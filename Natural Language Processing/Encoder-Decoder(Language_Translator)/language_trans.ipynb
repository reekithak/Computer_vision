{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/french-english/_about.txt\n/kaggle/input/french-english/fra.txt\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input , LSTM , Dense\nimport numpy as np\n\n\n\nbatch_size = 64\n\nepochs = 100\n\nlatent_dim =256 #size of feature vector\n\nnum_samples = 10000\n\ndata_path = '/kaggle/input/french-english/fra.txt'","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_texts = []\ntarget_texts = []\n\ninput_characters = set()\ntarget_characters = set()\n\nwith open(data_path,'r',encoding='utf-8') as f:\n    lines = f.read().split('\\n')\n    \nfor line in lines[:min(num_samples,len(lines)-1)]:\n    input_text , target_text , _ = line.split('\\t')\n    \n    target_text = '\\t' + target_text + '\\n'\n    input_texts.append(input_text)\n    target_texts.append(target_text)\n    \n    for char in input_text:\n        if char not in input_characters:\n            input_characters.add(char)\n    \n    \n    for char in target_text:\n        if char not in target_characters:\n            target_characters.add(char)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_texts[:5],target_texts[:5]","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"(['Go.', 'Hi.', 'Hi.', 'Run!', 'Run!'],\n ['\\tVa !\\n',\n  '\\tSalut !\\n',\n  '\\tSalut.\\n',\n  '\\tCours\\u202f!\\n',\n  '\\tCourez\\u202f!\\n'])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_characters = sorted(list(input_characters))\n\ntarget_characters = sorted(list(target_characters))\n\nnum_encoder_tokens = len(input_characters)\n\nnum_decoder_tokens = len(target_characters)\n\nmax_encoder_seq_length = max([len(txt) for txt in input_texts])\n\nmax_decoder_seq_length = max([len(txt) for txt in target_texts])","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"number of samples\" ,  len(input_texts))\nprint(\"max input seq length \",max_encoder_seq_length)\nprint('max decoder seq length',max_decoder_seq_length)","execution_count":10,"outputs":[{"output_type":"stream","text":"number of samples 10000\nmax input seq length  15\nmax decoder seq length 59\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_token_index = dict([(char,i) for i,char in enumerate(input_characters)])\n\ntarget_token_index = dict([(char,i) for i,char in enumerate(target_characters)])","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_token_index , target_token_index","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"({' ': 0,\n  '!': 1,\n  '$': 2,\n  '%': 3,\n  '&': 4,\n  \"'\": 5,\n  ',': 6,\n  '-': 7,\n  '.': 8,\n  '0': 9,\n  '1': 10,\n  '2': 11,\n  '3': 12,\n  '5': 13,\n  '6': 14,\n  '7': 15,\n  '8': 16,\n  '9': 17,\n  ':': 18,\n  '?': 19,\n  'A': 20,\n  'B': 21,\n  'C': 22,\n  'D': 23,\n  'E': 24,\n  'F': 25,\n  'G': 26,\n  'H': 27,\n  'I': 28,\n  'J': 29,\n  'K': 30,\n  'L': 31,\n  'M': 32,\n  'N': 33,\n  'O': 34,\n  'P': 35,\n  'Q': 36,\n  'R': 37,\n  'S': 38,\n  'T': 39,\n  'U': 40,\n  'V': 41,\n  'W': 42,\n  'Y': 43,\n  'a': 44,\n  'b': 45,\n  'c': 46,\n  'd': 47,\n  'e': 48,\n  'f': 49,\n  'g': 50,\n  'h': 51,\n  'i': 52,\n  'j': 53,\n  'k': 54,\n  'l': 55,\n  'm': 56,\n  'n': 57,\n  'o': 58,\n  'p': 59,\n  'q': 60,\n  'r': 61,\n  's': 62,\n  't': 63,\n  'u': 64,\n  'v': 65,\n  'w': 66,\n  'x': 67,\n  'y': 68,\n  'z': 69,\n  'é': 70},\n {'\\t': 0,\n  '\\n': 1,\n  ' ': 2,\n  '!': 3,\n  '$': 4,\n  '%': 5,\n  '&': 6,\n  \"'\": 7,\n  '(': 8,\n  ')': 9,\n  ',': 10,\n  '-': 11,\n  '.': 12,\n  '0': 13,\n  '1': 14,\n  '2': 15,\n  '3': 16,\n  '5': 17,\n  '8': 18,\n  '9': 19,\n  ':': 20,\n  '?': 21,\n  'A': 22,\n  'B': 23,\n  'C': 24,\n  'D': 25,\n  'E': 26,\n  'F': 27,\n  'G': 28,\n  'H': 29,\n  'I': 30,\n  'J': 31,\n  'K': 32,\n  'L': 33,\n  'M': 34,\n  'N': 35,\n  'O': 36,\n  'P': 37,\n  'Q': 38,\n  'R': 39,\n  'S': 40,\n  'T': 41,\n  'U': 42,\n  'V': 43,\n  'Y': 44,\n  'a': 45,\n  'b': 46,\n  'c': 47,\n  'd': 48,\n  'e': 49,\n  'f': 50,\n  'g': 51,\n  'h': 52,\n  'i': 53,\n  'j': 54,\n  'k': 55,\n  'l': 56,\n  'm': 57,\n  'n': 58,\n  'o': 59,\n  'p': 60,\n  'q': 61,\n  'r': 62,\n  's': 63,\n  't': 64,\n  'u': 65,\n  'v': 66,\n  'w': 67,\n  'x': 68,\n  'y': 69,\n  'z': 70,\n  '\\xa0': 71,\n  '«': 72,\n  '»': 73,\n  'À': 74,\n  'Ç': 75,\n  'É': 76,\n  'Ê': 77,\n  'à': 78,\n  'â': 79,\n  'ç': 80,\n  'è': 81,\n  'é': 82,\n  'ê': 83,\n  'ë': 84,\n  'î': 85,\n  'ï': 86,\n  'ô': 87,\n  'ù': 88,\n  'û': 89,\n  'œ': 90,\n  '\\u2009': 91,\n  '’': 92,\n  '\\u202f': 93})"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#onehot encoder\nencoder_input_data = np.zeros((len(input_texts),max_encoder_seq_length,num_encoder_tokens),dtype='float32')\n\ndecoder_input_data = np.zeros((len(input_texts),max_decoder_seq_length,num_decoder_tokens),dtype='float32')\n\n\ndecoder_target_data = np.zeros((len(input_texts),max_decoder_seq_length,num_decoder_tokens),dtype='float32')\n","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#one hot implementation - from above (continuation)\n\nfor i ,(input_text,target_text) in enumerate(zip(input_texts,target_texts)):\n    \n    for t,char in enumerate(input_text):\n        encoder_input_data[i,t,input_token_index[char]] = 1. \n    encoder_input_data[i,t+1:,input_token_index[' ']] =1.\n    \n    for t,char in enumerate(target_text):\n        decoder_input_data[i,t,target_token_index[char]] = 1.\n        \n        if t>0:\n            \n            decoder_target_data[i,t-1,target_token_index[char]] = 1.\n    \n    decoder_input_data[i,t+1:,target_token_index[' ']] = 1.\n    decoder_target_data[i,t:,target_token_index[' ']] = 1.","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_input_data[0].shape","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"(15, 71)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#building model\n\n#ENCODER\nencoder_inputs = Input(shape=(None,num_encoder_tokens))\n\nencoder = LSTM(latent_dim,return_state=True)\n\nencoder_outputs , state_h , state_c = encoder(encoder_inputs)\n# we dont need the encoder outputs one by one so we ignore it and only consider the hidden states\n\nencoder_states = [state_h,state_c]\n\n\n\n#DECODER\ndecoder_inputs = Input(shape=(None,num_decoder_tokens))\n\ndecoder_lstm = LSTM(latent_dim,return_sequences = True, return_state=True)\n\ndecoder_outputs , __ , _ = decoder_lstm(decoder_inputs,\n                                 initial_state=encoder_states)\n\n#FINAL LAYERS\n\ndecoder_dense = Dense(num_decoder_tokens,activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model([encoder_inputs,decoder_inputs],decoder_outputs)\n\nmodel.compile(optimizer='rmsprop' , loss = 'categorical_crossentropy',metrics=['accuracy'])\nmodel.summary()","execution_count":23,"outputs":[{"output_type":"stream","text":"Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_6 (InputLayer)            [(None, None, 71)]   0                                            \n__________________________________________________________________________________________________\ninput_7 (InputLayer)            [(None, None, 94)]   0                                            \n__________________________________________________________________________________________________\nlstm_5 (LSTM)                   [(None, 256), (None, 335872      input_6[0][0]                    \n__________________________________________________________________________________________________\nlstm_6 (LSTM)                   [(None, None, 256),  359424      input_7[0][0]                    \n                                                                 lstm_5[0][1]                     \n                                                                 lstm_5[0][2]                     \n__________________________________________________________________________________________________\ndense (Dense)                   (None, None, 94)     24158       lstm_6[0][0]                     \n==================================================================================================\nTotal params: 719,454\nTrainable params: 719,454\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit([encoder_input_data,decoder_input_data],\n         decoder_target_data,\n         batch_size=batch_size,\n         epochs=120,\n         validation_split=0.2,\n         )\n\nmodel.save('s2s')","execution_count":24,"outputs":[{"output_type":"stream","text":"Epoch 1/120\n125/125 [==============================] - 2s 20ms/step - loss: 1.1784 - accuracy: 0.7214 - val_loss: 1.1081 - val_accuracy: 0.7113\nEpoch 2/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.8743 - accuracy: 0.7673 - val_loss: 0.8520 - val_accuracy: 0.7610\nEpoch 3/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.7051 - accuracy: 0.8033 - val_loss: 0.7308 - val_accuracy: 0.7897\nEpoch 4/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.6123 - accuracy: 0.8223 - val_loss: 0.6651 - val_accuracy: 0.8038\nEpoch 5/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.5564 - accuracy: 0.8375 - val_loss: 0.6287 - val_accuracy: 0.8134\nEpoch 6/120\n125/125 [==============================] - 2s 15ms/step - loss: 0.5161 - accuracy: 0.8487 - val_loss: 0.6007 - val_accuracy: 0.8207\nEpoch 7/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.4851 - accuracy: 0.8568 - val_loss: 0.5761 - val_accuracy: 0.8303\nEpoch 8/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.4597 - accuracy: 0.8642 - val_loss: 0.5502 - val_accuracy: 0.8359\nEpoch 9/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.4379 - accuracy: 0.8702 - val_loss: 0.5362 - val_accuracy: 0.8422\nEpoch 10/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.4185 - accuracy: 0.8755 - val_loss: 0.5226 - val_accuracy: 0.8457\nEpoch 11/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.4012 - accuracy: 0.8803 - val_loss: 0.5063 - val_accuracy: 0.8493\nEpoch 12/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.3850 - accuracy: 0.8851 - val_loss: 0.4980 - val_accuracy: 0.8528\nEpoch 13/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.3699 - accuracy: 0.8890 - val_loss: 0.4870 - val_accuracy: 0.8558\nEpoch 14/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.3557 - accuracy: 0.8929 - val_loss: 0.4800 - val_accuracy: 0.8586\nEpoch 15/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.3428 - accuracy: 0.8971 - val_loss: 0.4781 - val_accuracy: 0.8604\nEpoch 16/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.3300 - accuracy: 0.9008 - val_loss: 0.4728 - val_accuracy: 0.8609\nEpoch 17/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.3181 - accuracy: 0.9038 - val_loss: 0.4673 - val_accuracy: 0.8623\nEpoch 18/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.3068 - accuracy: 0.9075 - val_loss: 0.4636 - val_accuracy: 0.8649\nEpoch 19/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.2960 - accuracy: 0.9107 - val_loss: 0.4651 - val_accuracy: 0.8648\nEpoch 20/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.2858 - accuracy: 0.9138 - val_loss: 0.4618 - val_accuracy: 0.8661\nEpoch 21/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.2761 - accuracy: 0.9167 - val_loss: 0.4583 - val_accuracy: 0.8679\nEpoch 22/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.2671 - accuracy: 0.9192 - val_loss: 0.4602 - val_accuracy: 0.8677\nEpoch 23/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.2577 - accuracy: 0.9218 - val_loss: 0.4563 - val_accuracy: 0.8689\nEpoch 24/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.2499 - accuracy: 0.9243 - val_loss: 0.4548 - val_accuracy: 0.8710\nEpoch 25/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.2415 - accuracy: 0.9268 - val_loss: 0.4599 - val_accuracy: 0.8697\nEpoch 26/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.2340 - accuracy: 0.9292 - val_loss: 0.4608 - val_accuracy: 0.8698\nEpoch 27/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.2263 - accuracy: 0.9313 - val_loss: 0.4624 - val_accuracy: 0.8708\nEpoch 28/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.2191 - accuracy: 0.9334 - val_loss: 0.4658 - val_accuracy: 0.8712\nEpoch 29/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.2125 - accuracy: 0.9351 - val_loss: 0.4671 - val_accuracy: 0.8715\nEpoch 30/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.2058 - accuracy: 0.9372 - val_loss: 0.4693 - val_accuracy: 0.8722\nEpoch 31/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.1996 - accuracy: 0.9394 - val_loss: 0.4709 - val_accuracy: 0.8726\nEpoch 32/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.1936 - accuracy: 0.9411 - val_loss: 0.4730 - val_accuracy: 0.8722\nEpoch 33/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1878 - accuracy: 0.9429 - val_loss: 0.4774 - val_accuracy: 0.8726\nEpoch 34/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1822 - accuracy: 0.9444 - val_loss: 0.4789 - val_accuracy: 0.8718\nEpoch 35/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1771 - accuracy: 0.9458 - val_loss: 0.4846 - val_accuracy: 0.8721\nEpoch 36/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.1718 - accuracy: 0.9476 - val_loss: 0.4843 - val_accuracy: 0.8726\nEpoch 37/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1671 - accuracy: 0.9491 - val_loss: 0.4889 - val_accuracy: 0.8734\nEpoch 38/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.1624 - accuracy: 0.9503 - val_loss: 0.4975 - val_accuracy: 0.8722\nEpoch 39/120\n125/125 [==============================] - 2s 15ms/step - loss: 0.1582 - accuracy: 0.9514 - val_loss: 0.4992 - val_accuracy: 0.8723\nEpoch 40/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1539 - accuracy: 0.9527 - val_loss: 0.5040 - val_accuracy: 0.8727\nEpoch 41/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1496 - accuracy: 0.9539 - val_loss: 0.5066 - val_accuracy: 0.8727\nEpoch 42/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1461 - accuracy: 0.9552 - val_loss: 0.5150 - val_accuracy: 0.8726\nEpoch 43/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1420 - accuracy: 0.9565 - val_loss: 0.5162 - val_accuracy: 0.8724\nEpoch 44/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1385 - accuracy: 0.9575 - val_loss: 0.5262 - val_accuracy: 0.8721\nEpoch 45/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.1348 - accuracy: 0.9586 - val_loss: 0.5283 - val_accuracy: 0.8720\nEpoch 46/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1318 - accuracy: 0.9596 - val_loss: 0.5296 - val_accuracy: 0.8720\nEpoch 47/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1284 - accuracy: 0.9601 - val_loss: 0.5377 - val_accuracy: 0.8711\nEpoch 48/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1256 - accuracy: 0.9611 - val_loss: 0.5385 - val_accuracy: 0.8724\nEpoch 49/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1225 - accuracy: 0.9620 - val_loss: 0.5432 - val_accuracy: 0.8718\nEpoch 50/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.1193 - accuracy: 0.9629 - val_loss: 0.5477 - val_accuracy: 0.8718\nEpoch 51/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1169 - accuracy: 0.9635 - val_loss: 0.5586 - val_accuracy: 0.8712\nEpoch 52/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.1140 - accuracy: 0.9646 - val_loss: 0.5609 - val_accuracy: 0.8709\nEpoch 53/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1117 - accuracy: 0.9649 - val_loss: 0.5613 - val_accuracy: 0.8720\nEpoch 54/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1093 - accuracy: 0.9658 - val_loss: 0.5703 - val_accuracy: 0.8717\nEpoch 55/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1070 - accuracy: 0.9664 - val_loss: 0.5745 - val_accuracy: 0.8714\nEpoch 56/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.1041 - accuracy: 0.9673 - val_loss: 0.5805 - val_accuracy: 0.8711\nEpoch 57/120\n","name":"stdout"},{"output_type":"stream","text":"125/125 [==============================] - 2s 13ms/step - loss: 0.1022 - accuracy: 0.9678 - val_loss: 0.5806 - val_accuracy: 0.8711\nEpoch 58/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.1002 - accuracy: 0.9685 - val_loss: 0.5885 - val_accuracy: 0.8712\nEpoch 59/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0981 - accuracy: 0.9688 - val_loss: 0.5936 - val_accuracy: 0.8713\nEpoch 60/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0959 - accuracy: 0.9696 - val_loss: 0.5963 - val_accuracy: 0.8705\nEpoch 61/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0942 - accuracy: 0.9700 - val_loss: 0.6009 - val_accuracy: 0.8705\nEpoch 62/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0923 - accuracy: 0.9705 - val_loss: 0.6038 - val_accuracy: 0.8706\nEpoch 63/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0903 - accuracy: 0.9711 - val_loss: 0.6148 - val_accuracy: 0.8703\nEpoch 64/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0885 - accuracy: 0.9716 - val_loss: 0.6124 - val_accuracy: 0.8706\nEpoch 65/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.0872 - accuracy: 0.9721 - val_loss: 0.6159 - val_accuracy: 0.8718\nEpoch 66/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0850 - accuracy: 0.9729 - val_loss: 0.6208 - val_accuracy: 0.8702\nEpoch 67/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0836 - accuracy: 0.9732 - val_loss: 0.6238 - val_accuracy: 0.8713\nEpoch 68/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0819 - accuracy: 0.9737 - val_loss: 0.6311 - val_accuracy: 0.8704\nEpoch 69/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.0805 - accuracy: 0.9741 - val_loss: 0.6336 - val_accuracy: 0.8708\nEpoch 70/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0787 - accuracy: 0.9744 - val_loss: 0.6396 - val_accuracy: 0.8707\nEpoch 71/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.0776 - accuracy: 0.9748 - val_loss: 0.6405 - val_accuracy: 0.8708\nEpoch 72/120\n125/125 [==============================] - 2s 15ms/step - loss: 0.0760 - accuracy: 0.9751 - val_loss: 0.6459 - val_accuracy: 0.8710\nEpoch 73/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0748 - accuracy: 0.9757 - val_loss: 0.6492 - val_accuracy: 0.8699\nEpoch 74/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.0737 - accuracy: 0.9759 - val_loss: 0.6520 - val_accuracy: 0.8712\nEpoch 75/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.0723 - accuracy: 0.9764 - val_loss: 0.6593 - val_accuracy: 0.8698\nEpoch 76/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0710 - accuracy: 0.9765 - val_loss: 0.6637 - val_accuracy: 0.8703\nEpoch 77/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0695 - accuracy: 0.9774 - val_loss: 0.6680 - val_accuracy: 0.8702\nEpoch 78/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.0684 - accuracy: 0.9774 - val_loss: 0.6703 - val_accuracy: 0.8708\nEpoch 79/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0670 - accuracy: 0.9778 - val_loss: 0.6720 - val_accuracy: 0.8707\nEpoch 80/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0662 - accuracy: 0.9782 - val_loss: 0.6759 - val_accuracy: 0.8708\nEpoch 81/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0648 - accuracy: 0.9786 - val_loss: 0.6817 - val_accuracy: 0.8701\nEpoch 82/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0641 - accuracy: 0.9786 - val_loss: 0.6847 - val_accuracy: 0.8699\nEpoch 83/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0629 - accuracy: 0.9790 - val_loss: 0.6928 - val_accuracy: 0.8699\nEpoch 84/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0616 - accuracy: 0.9795 - val_loss: 0.6935 - val_accuracy: 0.8697\nEpoch 85/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.0612 - accuracy: 0.9795 - val_loss: 0.6910 - val_accuracy: 0.8697\nEpoch 86/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0600 - accuracy: 0.9800 - val_loss: 0.6931 - val_accuracy: 0.8699\nEpoch 87/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0590 - accuracy: 0.9800 - val_loss: 0.7007 - val_accuracy: 0.8702\nEpoch 88/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.0581 - accuracy: 0.9804 - val_loss: 0.7079 - val_accuracy: 0.8696\nEpoch 89/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0573 - accuracy: 0.9805 - val_loss: 0.7032 - val_accuracy: 0.8702\nEpoch 90/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0561 - accuracy: 0.9810 - val_loss: 0.7170 - val_accuracy: 0.8690\nEpoch 91/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.0555 - accuracy: 0.9811 - val_loss: 0.7228 - val_accuracy: 0.8699\nEpoch 92/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0548 - accuracy: 0.9811 - val_loss: 0.7260 - val_accuracy: 0.8691\nEpoch 93/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0539 - accuracy: 0.9813 - val_loss: 0.7251 - val_accuracy: 0.8691\nEpoch 94/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0533 - accuracy: 0.9818 - val_loss: 0.7283 - val_accuracy: 0.8697\nEpoch 95/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0523 - accuracy: 0.9819 - val_loss: 0.7317 - val_accuracy: 0.8695\nEpoch 96/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0517 - accuracy: 0.9823 - val_loss: 0.7318 - val_accuracy: 0.8692\nEpoch 97/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0506 - accuracy: 0.9824 - val_loss: 0.7367 - val_accuracy: 0.8686\nEpoch 98/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.0502 - accuracy: 0.9825 - val_loss: 0.7408 - val_accuracy: 0.8693\nEpoch 99/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0494 - accuracy: 0.9828 - val_loss: 0.7440 - val_accuracy: 0.8688\nEpoch 100/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0490 - accuracy: 0.9829 - val_loss: 0.7447 - val_accuracy: 0.8682\nEpoch 101/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0481 - accuracy: 0.9833 - val_loss: 0.7527 - val_accuracy: 0.8691\nEpoch 102/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0475 - accuracy: 0.9833 - val_loss: 0.7538 - val_accuracy: 0.8689\nEpoch 103/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0469 - accuracy: 0.9837 - val_loss: 0.7540 - val_accuracy: 0.8690\nEpoch 104/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0464 - accuracy: 0.9837 - val_loss: 0.7651 - val_accuracy: 0.8684\nEpoch 105/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.0458 - accuracy: 0.9837 - val_loss: 0.7648 - val_accuracy: 0.8682\nEpoch 106/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0452 - accuracy: 0.9839 - val_loss: 0.7608 - val_accuracy: 0.8686\nEpoch 107/120\n125/125 [==============================] - 2s 16ms/step - loss: 0.0447 - accuracy: 0.9842 - val_loss: 0.7722 - val_accuracy: 0.8687\nEpoch 108/120\n125/125 [==============================] - 2s 16ms/step - loss: 0.0442 - accuracy: 0.9843 - val_loss: 0.7675 - val_accuracy: 0.8692\nEpoch 109/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0435 - accuracy: 0.9845 - val_loss: 0.7783 - val_accuracy: 0.8674\nEpoch 110/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0431 - accuracy: 0.9847 - val_loss: 0.7742 - val_accuracy: 0.8690\nEpoch 111/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.0425 - accuracy: 0.9850 - val_loss: 0.7800 - val_accuracy: 0.8691\nEpoch 112/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0420 - accuracy: 0.9850 - val_loss: 0.7786 - val_accuracy: 0.8688\nEpoch 113/120\n","name":"stdout"},{"output_type":"stream","text":"125/125 [==============================] - 2s 13ms/step - loss: 0.0420 - accuracy: 0.9850 - val_loss: 0.7813 - val_accuracy: 0.8694\nEpoch 114/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0409 - accuracy: 0.9854 - val_loss: 0.7874 - val_accuracy: 0.8696\nEpoch 115/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0408 - accuracy: 0.9853 - val_loss: 0.7875 - val_accuracy: 0.8688\nEpoch 116/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0403 - accuracy: 0.9853 - val_loss: 0.7909 - val_accuracy: 0.8682\nEpoch 117/120\n125/125 [==============================] - 2s 14ms/step - loss: 0.0403 - accuracy: 0.9852 - val_loss: 0.7991 - val_accuracy: 0.8679\nEpoch 118/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0396 - accuracy: 0.9857 - val_loss: 0.8040 - val_accuracy: 0.8678\nEpoch 119/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0390 - accuracy: 0.9858 - val_loss: 0.7991 - val_accuracy: 0.8678\nEpoch 120/120\n125/125 [==============================] - 2s 13ms/step - loss: 0.0387 - accuracy: 0.9859 - val_loss: 0.8052 - val_accuracy: 0.8680\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"lang_trans.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nmodel = keras.models.load_model(\"s2s\")\n\nencoder_inputs = model.input[0]  # input_1\nencoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\nencoder_states = [state_h_enc, state_c_enc]\nencoder_model = keras.Model(encoder_inputs, encoder_states)\n\ndecoder_inputs = model.input[1]  # input_2\ndecoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_3\")\ndecoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4\")\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_lstm = model.layers[3]\ndecoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n    decoder_inputs, initial_state=decoder_states_inputs\n)\ndecoder_states = [state_h_dec, state_c_dec]\ndecoder_dense = model.layers[4]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = keras.Model(\n    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n)\n\n# Reverse-lookup token index to decode sequences back to\n# something readable.\nreverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\nreverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n\n\ndef decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1, num_decoder_tokens))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = \"\"\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_char_index[sampled_token_index]\n        decoded_sentence += sampled_char\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1, num_decoder_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.0\n\n        # Update states\n        states_value = [h, c]\n    return decoded_sentence","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for seq_index in range(20):\n    # Take one sequence (part of the training set)\n    # for trying out decoding.\n    input_seq = encoder_input_data[seq_index : seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    print(\"-\")\n    print(\"Input sentence:\", input_texts[seq_index])\n    print(\"Decoded sentence:\", decoded_sentence)","execution_count":28,"outputs":[{"output_type":"stream","text":"-\nInput sentence: Go.\nDecoded sentence: Va !\n\n-\nInput sentence: Hi.\nDecoded sentence: Salut.\n\n-\nInput sentence: Hi.\nDecoded sentence: Salut.\n\n-\nInput sentence: Run!\nDecoded sentence: Courez !\n\n-\nInput sentence: Run!\nDecoded sentence: Courez !\n\n-\nInput sentence: Who?\nDecoded sentence: Qui ?\n\n-\nInput sentence: Wow!\nDecoded sentence: Ça alors !\n\n-\nInput sentence: Fire!\nDecoded sentence: Au feu !\n\n-\nInput sentence: Help!\nDecoded sentence: À l'aide !\n\n-\nInput sentence: Jump.\nDecoded sentence: Saute.\n\n-\nInput sentence: Stop!\nDecoded sentence: Arrête-toi !\n\n-\nInput sentence: Stop!\nDecoded sentence: Arrête-toi !\n\n-\nInput sentence: Stop!\nDecoded sentence: Arrête-toi !\n\n-\nInput sentence: Wait!\nDecoded sentence: Attendez !\n\n-\nInput sentence: Wait!\nDecoded sentence: Attendez !\n\n-\nInput sentence: Go on.\nDecoded sentence: Poursuivez.\n\n-\nInput sentence: Go on.\nDecoded sentence: Poursuivez.\n\n-\nInput sentence: Go on.\nDecoded sentence: Poursuivez.\n\n-\nInput sentence: Hello!\nDecoded sentence: Salut !\n\n-\nInput sentence: Hello!\nDecoded sentence: Salut !\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}